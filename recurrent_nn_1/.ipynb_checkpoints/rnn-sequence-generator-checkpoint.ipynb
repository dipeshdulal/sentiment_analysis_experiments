{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using `numpy` to create recurrent neural networks (video from siraj raval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['m', 'i', '?', 'C', 'l', 'N', '(', 'P', ':', 'L', ';', '\\n', 'U', 'o', ')', 'z', \"'\", 's', 'F', 'q', 'r', 'a', 'y', 'W', 'G', 'e', 'c', 'E', 'u', 'n', '\"', 'M', 'I', 'V', 'w', 'p', '.', 'D', ',', 'S', '-', 'Q', 'k', 'O', 'B', 'd', 'ç', 'T', 'h', 't', 'b', ' ', 'x', '!', 'A', 'j', 'H', 'g', 'Y', 'J', 'v']\n"
     ]
    }
   ],
   "source": [
    "data = open('nepali.txt', 'r').read()\n",
    "\n",
    "chars = list(set(data)) # convert into unique characters\n",
    "print(chars[1:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 118560 chars and 62 unique chars\n"
     ]
    }
   ],
   "source": [
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d chars and %d unique chars'%(data_size, vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--dictionary to encode and decode char to an int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'f', 1: 'm', 2: 'i', 3: '?', 4: 'C', 5: 'l', 6: 'N', 7: '(', 8: 'P', 9: ':', 10: 'L', 11: ';', 12: '\\n', 13: 'U', 14: 'o', 15: ')', 16: 'z', 17: \"'\", 18: 's', 19: 'F', 20: 'q', 21: 'r', 22: 'a', 23: 'y', 24: 'W', 25: 'G', 26: 'e', 27: 'c', 28: 'E', 29: 'u', 30: 'n', 31: '\"', 32: 'M', 33: 'I', 34: 'V', 35: 'w', 36: 'p', 37: '.', 38: 'D', 39: ',', 40: 'S', 41: '-', 42: 'Q', 43: 'k', 44: 'O', 45: 'B', 46: 'd', 47: 'ç', 48: 'T', 49: 'h', 50: 't', 51: 'b', 52: ' ', 53: 'x', 54: '!', 55: 'A', 56: 'j', 57: 'H', 58: 'g', 59: 'Y', 60: 'J', 61: 'v'}\n"
     ]
    }
   ],
   "source": [
    "# char_to_integer and integer_to_char\n",
    "char_to_number = { ch:i for i,ch in enumerate(chars) }\n",
    "number_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "print(number_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# create one hot encoded chars vectors\n",
    "import numpy as np\n",
    "\n",
    "vector_for_char_a = np.zeros((vocab_size, 1))\n",
    "vector_for_char_a[char_to_number['a']] = 1\n",
    "print(vector_for_char_a.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define the `model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first define hyperparameters\n",
    "hidden_size = 100\n",
    "seq_length = 25\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# now define model parameters (weights)\n",
    "w_xh = np.random.randn(hidden_size, vocab_size) * 0.01   # input hidden weight matrix\n",
    "w_hh = np.random.randn(hidden_size, hidden_size) * 0.01 # recurrent weight matrix\n",
    "\n",
    "w_hy = np.random.randn(vocab_size, hidden_size) * 0.01 # recurrent weight matrix\n",
    "bh = np.zeros((hidden_size, 1))\n",
    "by = np.zeros((vocab_size, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`xs` for inputs, `hs` for hidden state values\n",
    "`ys` for targets and `ps` for normalized probability values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we make the model  \n",
    "# for forward pass \n",
    "def lossFun(inputs, targets, hprev):\n",
    "    xs, hs, ys, ps = {}, {}, {}, {}\n",
    "    hs[-1] = np.copy(hprev)\n",
    "    loss = 0\n",
    "    \n",
    "    # now forward pass \n",
    "    for t in range(len(inputs)):\n",
    "        xs[t] = np.zeros((vocab_size, 1))\n",
    "        xs[t][inputs[t]] = 1 # one hot encoding for inputs\n",
    "        \n",
    "        hs[t] = np.tanh(np.dot(w_xh, xs[t]) + np.dot(w_hh, hs[t-1]) + bh)\n",
    "        ys[t] = np.dot(w_hy, hs[t]) + by\n",
    "        \n",
    "        ps[t] = np.exp(ys[t]) / np.sum( np.exp(ys[t]) ) #softmax\n",
    "        loss += -np.log(ps[t][targets[t], 0])\n",
    "        \n",
    "    # now backward_pass\n",
    "    dw_xh, dw_hh, dw_hy = np.zeros_like(w_xh), np.zeros_like(w_hh), np.zeros_like(w_hy)\n",
    "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "    dh_next = np.zeros_like(hs[0])\n",
    "    \n",
    "    for t in reversed(range(len(inputs))):\n",
    "        dy = np.copy(ps[t])\n",
    "        \n",
    "        # gradient : prob - 1 so,\n",
    "        dy[targets[t]] -= 1\n",
    "        \n",
    "        dw_hy += np.dot(dy, hs[t].T)\n",
    "        dby += dy\n",
    "        \n",
    "        dh = np.dot(w_hy.T, dy) + dh_next\n",
    "        dhraw = (1-hs[t] * hs[t]) * dh\n",
    "        dbh += dhraw\n",
    "        \n",
    "        dw_xh += np.dot(dhraw, xs[t].T)\n",
    "        dw_hh += np.dot(dhraw, hs[t-1].T)\n",
    "        dh_next = np.dot(w_hh.T, dhraw)\n",
    "        \n",
    "    for dparam in [dw_xh, dw_hh, dw_hy, dbh, dby]:\n",
    "        np.clip(dparam, -5, 5, out=dparam)\n",
    "    \n",
    "    return loss, dw_xh, dw_hh, dw_hy, dbh, dby, hs[len(inputs)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      " GzxJAYhDBfHbçAGO T DBo iiJ?DNçoltk\"woC\"IDVsQJdMz\"Ht)-?.QJkU)uqBqxfWfgM.PtW-Jk,AWmF-ob;JnJpdh,W'n(s .HuW:iajHsFBTsu'iVILf.COktN'wG,j)TVA \n",
      ";yp n,NEAApeYwcd?Hc;'(FPvLSMNL(wdTC?hVv-AJkhWt,N\"Mn;Eq'a,tz;CTm \n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "def sample(h, seed_ix, n):\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[seed_ix] = 1\n",
    "    \n",
    "    ixes = [] # list to store the generated characters \n",
    "    for t in range(n):\n",
    "        h = np.tanh(np.dot(w_xh, x) + np.dot(w_hh, h) + bh)\n",
    "        y = np.dot(w_hy, h) + by\n",
    "        \n",
    "        p = np.exp(y) / np.sum(np.exp(y))\n",
    "        ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        \n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[ix] = 1\n",
    "        ixes.append(ix)\n",
    "        \n",
    "    txt = ''.join(number_to_char[ix] for ix in ixes)\n",
    "    print('---\\n %s \\n-----' % txt)\n",
    "\n",
    "hprev = np.zeros((hidden_size, 1))\n",
    "sample(hprev, char_to_number['a'], 200)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[44, 30, 26, 52, 1, 14, 21, 30, 2, 30, 58, 39, 52, 35, 49, 26, 30, 52, 25, 21, 26, 58, 14, 21, 52] [30, 26, 52, 1, 14, 21, 30, 2, 30, 58, 39, 52, 35, 49, 26, 30, 52, 25, 21, 26, 58, 14, 21, 52, 40]\n"
     ]
    }
   ],
   "source": [
    "p = 0\n",
    "inputs = [ char_to_number[ch] for ch in data[p:p+seq_length] ]\n",
    "targets = [ char_to_number[ch] for ch in data[p+1:p+seq_length+1] ]\n",
    "print(inputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, loss: 103.178362\n",
      "---\n",
      " .AJSd\"o'zxGlAa,MADv(NqlWfrjIHL.d.SMLuuDIo:uUvhytcG\"LD)gztCq)YVyAsmNO,JdYNAGS i-WtPGNmeVOPxP\"IiQ:gTtfGqvJ-cmFwLnUGy:NPdA'(rmoJ\"Viub.MBCAsq Lu.QxiIMCL;enwa p.v;Br)UIUzaTSeOPAOwcvf:inSP?)pPUJjçSDYO! fWbS \n",
      "-----\n",
      "iter 1000, loss: 83.983083\n",
      "---\n",
      " ls wolt theion che himy fe hmridz reu\"hr pe s inBt wano hasw pet the ham  gant chaslss hheuss ans the wing bn saul wisuuunt t ieaed ibhen ar. wGasvhe tfFade onrpl toeor ansdiughom te npe oyr b tocces  \n",
      "-----\n",
      "iter 2000, loss: 68.527710\n",
      "---\n",
      " tigotherito waut nhedlmes tio Ghe.r wGour Auerof i(let ant, ther dilre hoctor whs dre hlasl akt on hesver rilo'ps tht hat ans hoved hurint toad hioned ab.cGrme ghatreio huteer. to ther ous as tocy rcu \n",
      "-----\n",
      "iter 3000, loss: 60.361233\n",
      "---\n",
      " apl hor sick nuts wir sony Cot torm coud hasind ther wanr Ghes tis uvever hip bus hik the shend thos are. Hhovepry hcinledualen\n",
      "was jporf as,h nner hiit puwhighind war, he touk wat; ibsdy fhe walt tha \n",
      "-----\n",
      "iter 4000, loss: 55.704229\n",
      "---\n",
      "  Hedict foaid avere.\n",
      "H, beted fustingimr nid ne; wond bom prewly he phat forebnafn thoy hirr in, at iad foin un lick wheraly dintim the the nomouly thoorllsiste wo haintir if kingrebeg. thivedonevat i \n",
      "-----\n",
      "iter 5000, loss: 54.500784\n",
      "---\n",
      " at wochowed, bomerparcis int saceve coth okt shen growat fustany; whas souply whimddes daves nas or and fpvede somarn, hien teke tind ham clas s shikr, fttay the cotr ace ceath ouldus as ull the ind l \n",
      "-----\n",
      "iter 6000, loss: 53.258351\n",
      "---\n",
      " s at bald, the to caet trond theith tad eas fhand thaid to Had the khim ned and lod stondt ineing cosh lat putry has no the frew that er has domer amer aut. Phoud upstis weveghy thas him besat's aris  \n",
      "-----\n",
      "iter 7000, loss: 51.015939\n",
      "---\n",
      "  the beete bingl sould enlego lay caere don, the the dured, band soretne wir hit the slingt -, the dectoulpang bing aw the thea, restrandst wasinjonstard tor lout thel wrpanged as no the wextleniate y \n",
      "-----\n",
      "iter 8000, loss: 49.794011\n",
      "---\n",
      " ould in hind sist rick tered nork wes atlegere to uppen to to lathaveily ongreashes and if it it or the rele sttiigh wort wack as bimuter the bavisay whe mreether feruteen her hey anad it himey Gath s \n",
      "-----\n",
      "iter 9000, loss: 49.302294\n",
      "---\n",
      " senat alt - was hinles, had, swather tha ciod evevey the packising it deto thine hay veit yowlat ior tu cisingr the suse rad Gregor has ruan. I tho lo it aicher hay bas mook an root; motle, h, leeatse \n",
      "-----\n",
      "iter 10000, loss: 49.823198\n",
      "---\n",
      " edren fored be shither move ghace fhar ha  he sobeint theme noon, de wiqus. qubout ploke, t sasthe whit wat of ha roow\" he ceowen And fo fren hem, whir to hame veited had of, hat alreel bat as that ou \n",
      "-----\n",
      "iter 11000, loss: 49.180617\n",
      "---\n",
      " d thf fored oncisider hamews of havenny. Anter bighing hind shut thatef had qlemse the his theveg. Gregonghor wock kca deop he thingot shat and his s.oH .NGr at the moulst simubut ron wadeadisist, mot \n",
      "-----\n",
      "iter 12000, loss: 47.663184\n",
      "---\n",
      " had but the nesty if the no warf sarmet but to ci, elingen, four ferrinenntolf sat that tha reing anting ill blessse yowen Gregor himse\" whe roo foreen and serke erreile cleshed her shonrile docher mo \n",
      "-----\n",
      "iter 13000, loss: 47.011042\n",
      "---\n",
      " xth, sameete, the of ole'lt herseines could lying they hinge hed aw to heas as brenone? hay and, hadly did it and net yibls the pouls and, anstor's liok. Gregor, hat ext ternirse whand evisly eamsere. \n",
      "-----\n",
      "iter 14000, loss: 46.957425\n",
      "---\n",
      " nen woon's wa cayed choto, thes wome she whame bacl, hishing to cight lere.\n",
      "\n",
      "The's inly andly ard lathas himimet of ther couk cust relk it thet senk cone dowme eesh Gregor bas ger. Gregor hinged her t \n",
      "-----\n",
      "iter 15000, loss: 47.676656\n",
      "---\n",
      " e moch off ankey selwbe he cuck, whes roke in shate, terinild at of a'd couted breevany but was if hilly an Gregars his was mole sous. The larwth to leat bmen the cocfle contient the rotiling anlo thi \n",
      "-----\n",
      "iter 16000, loss: 46.599462\n",
      "---\n",
      " st, geser'blasimen hey noom frat qpoving about hick it had the ther the rove be the oun lass the mat gughay waull this thetedis waid shed lmouthels was the dersing that urreapd was ofr is ardous mamis \n",
      "-----\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-5b5ec1e294d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m       \u001b[0;31m# forward seq_length characters through the net and fetch gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWxh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWhh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWhy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdbh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdby\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlossFun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhprev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0msmooth_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmooth_loss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.999\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-9c77e03f1396>\u001b[0m in \u001b[0;36mlossFun\u001b[0;34m(inputs, targets, hprev)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_hy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mby\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;31m#softmax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m   1880\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1881\u001b[0m     return _methods._sum(a, axis=axis, dtype=dtype,\n\u001b[0;32m-> 1882\u001b[0;31m                          out=out, **kwargs)\n\u001b[0m\u001b[1;32m   1883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_sum\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_prod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(w_xh), np.zeros_like(w_hh), np.zeros_like(w_hy)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad                                                                                                                \n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0                                                                                                                        \n",
    "while n<=1000*100:\n",
    "    if p+seq_length+1 >= len(data) or n == 0:\n",
    "        hprev = np.zeros((hidden_size,1)) # reset RNN memory                                                                                                                                      \n",
    "        p = 0 # go from start of data                                                                                                                                                             \n",
    "    inputs = [char_to_number[ch] for ch in data[p:p+seq_length]]\n",
    "    targets = [char_to_number[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "      # forward seq_length characters through the net and fetch gradient                                                                                                                          \n",
    "    loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "\n",
    "    if n % 1000 == 0:\n",
    "        print('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "        sample(hprev, inputs[0], 200)\n",
    "\n",
    "  # perform parameter update with Adagrad                                                                                                                                                     \n",
    "    for param, dparam, mem in zip([w_xh, w_hh, w_hy, bh, by],\n",
    "                                [dWxh, dWhh, dWhy, dbh, dby],\n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update                                                                                                                   \n",
    "\n",
    "    p += seq_length # move data pointer                                                                                                                                                         \n",
    "    n += 1 # iteration counter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
